import glob
import nltk
nltk.download('popular');
from nltk.corpus import stopwords
from nltk import word_tokenize
import string
from collections import Counter
import numpy as np
from collections import OrderedDict

"""# Reading Docs"""

f=open("file.txt")
docs=f.readlines()
docs=[i.strip() for i in docs]
doc_dict={"docs "+str(i):doc for i,doc in enumerate(docs)}   
print(doc_dict)

M = len(docs)

"""# Remove The Stop Words And Punctuations

"""

stop = stopwords.words('english') + list(string.punctuation) + ['\n']
wordList = []
for doc in doc_dict.values():
    for word in word_tokenize(doc.lower().strip()): 
        if not word in stop:
            wordList.append(word)
print(wordList)

vocab = list(set(wordList))

"""# Find The Frequency Of Words In Each Document. 
returns a dictionary with 'key' as document name and its 'value' as dicionary, containing a word as 'key' and its frequency as 'value'
"""

tf_docs = {}
for doc_id in doc_dict.keys():
    tf_docs[doc_id] = {}

for word in vocab:
    for doc_id,doc in doc_dict.items():
        tf_docs[doc_id][word] = doc.count(word)
print(tf_docs)

"""# Document Frequency
To count the document frequency, vocabulary of words and dictionary of collection of documents if passed to wordDocFre which returns a dictionary containing a word as 'key' and its document frequency as 'value' of dictionary.

"""

df = {}
for word in vocab:
    frq = 0
    for doc in doc_dict.values():
        if word in word_tokenize(doc.lower().strip()):
            frq = frq + 1
    df[word] = frq
print(df)

"""# Inverse Document Frequency
* Takes vocabulary of words, dictionary of document frequency and number of documents
* Returns a dictionary containing inverse DF in which a word is key and its idf score is its value.

"""

idf= {} 
for word in vocab:     
    idf[word] = np.log2((M+1) / df[word])
print(idf)

"""# TF-IDF Function"""

tf_idf_scr = {}
for doc_id in doc_dict.keys():
    tf_idf_scr[doc_id] = {}
for word in vocab:
    for doc_id,doc in doc_dict.items():
        tf_idf_scr[doc_id][word] = tf_docs[doc_id][word] * idf[word]
print(tf_idf_scr)

query = 'professional'

query_vocab = []
for word in query.split():
    if word not in query_vocab:
        query_vocab.append(word)

query_wc = {}
for word in query_vocab:
    query_wc[word] = query.lower().split().count(word)

relevance_scores = {}
for doc_id in doc_dict.keys():
    score = 0
    for word in query_vocab:
        score += query_wc[word] * tf_idf_scr[doc_id][word]
    relevance_scores[doc_id] = score
sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))
top_5 = {k: sorted_value[k] for k in list(sorted_value)[:5]}
print(top_5)

