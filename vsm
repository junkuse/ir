# -*- coding: utf-8 -*-
"""Vector_Space_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HgNgn1lFjXfWhuni8R5UuTRxFrafMYip

**Objective:** 

*Objective of this project is to implement the Vector Space Model (VSM). Dataset contains a collection of about 22,000 text files.*

*Assigned task was to implement a VSM and given a query, program should return top 5 documents related to the query.*

**Bellow cell imports all the necessary libraries**
"""

import glob
import nltk
nltk.download('popular');
from nltk.corpus import stopwords
from nltk import word_tokenize
import string
from collections import Counter
import numpy as np
from collections import OrderedDict

"""**give_path: **function** **

In below cell, give_path() takes path of the folder containing all the documents.
give_path() then read all the documents and store content of each document (in string format) as 'value' of dictionary and it's file name as 'key' in the dictionary.
This function the returns the dictionary containing all documents.
"""

def give_path(fld_path):
  f=open(fld_path)
  docs=f.readlines()
  docs=[i.strip() for i in docs]
  docsdict={"docs "+str(i):doc for i,doc in enumerate(docs)}    
  return docsdict

"""**wordList_removePuncs: **function** **

In the below cell, two functions wordList() and removePuncs() functions have been combined as wordList_removePuncs.
This function is passed with dictionary of all documents (returned by give_path()). Function then remove the stop words and punctuations and returns a list of all the words in our collection of documents. This list contains the redundant words.
"""

def wordList_removePuncs(doc_dict):
    stop = stopwords.words('english') + list(string.punctuation) + ['\n']
    wordList = []
    for doc in doc_dict.values():
        for word in word_tokenize(doc.lower().strip()): 
            if not word in stop:
                wordList.append(word)
    return wordList

"""**termFrequencyInDoc: **function** **

After the above function returns a list of all words in the collection of documents, vocabulary of words is made (I'm making vocabulary in 'main'). This vocabulary and collection of documents in dictionary form (returned by give_path) is passed to below function (termFrequencyInDoc) to find the frequency of words in each documet. This function returns a dictionary in which 'key' is the document name and its 'value' is another dicionary, containing a word as 'key' and its frequency as 'value'. It's actually a dictionary within a dictionary.
"""

def termFrequencyInDoc(vocab, doc_dict):
    tf_docs = {}
    for doc_id in doc_dict.keys():
        tf_docs[doc_id] = {}
    
    for word in vocab:
        for doc_id,doc in doc_dict.items():
            tf_docs[doc_id][word] = doc.count(word)
    return tf_docs

"""**wordDocFre: **function** **

To count the document frequency, vocabulary of words and dictionary of collection of documents if passed to wordDocFre which returns a dictionary containing a word as 'key' and its document frequency as 'value' of dictionary.
"""

def wordDocFre(vocab, doc_dict):
    df = {}
    for word in vocab:
        frq = 0
        for doc in doc_dict.values():
#             if word in doc.lower().split():
            if word in word_tokenize(doc.lower().strip()):
                frq = frq + 1
        df[word] = frq
    return df

"""**inverseDocFre: **function** **

inverseDocFre calculates the inverse document frequency. It takes vocabulary of words, dictionary of document frequency (returned by wordDocFre) and number of documents in our collection. It returns a dictionary containing inverse DF in which a word is key and its idf score is its value.
"""

def inverseDocFre(vocab,doc_fre,length):
    idf= {} 
    for word in vocab:     
        idf[word] = np.log2((length+1) / doc_fre[word])
    return idf

"""**tfidf: **function** **

Function tfidf takes 4 arguments:
1. vocabular of words 
2. term frequencies: which is passed in form of a dictionary
3. Inverse DF: passed in form of a dictionary
4. Collection of all docs passed in form of a dictionary

It returns a dictionary, which is again a dictionary within a dictionary like in case of TF, but it contains score values.
"""

def tfidf(vocab,tf,idf_scr,doc_dict):
    tf_idf_scr = {}
    for doc_id in doc_dict.keys():
        tf_idf_scr[doc_id] = {}
    for word in vocab:
        for doc_id,doc in doc_dict.items():
            tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]
    return tf_idf_scr

"""**vectorSpaceModel: **function** **

To find the relevant documents related to query, pass the query to function alonwith collection of documents (dictionary) and tf-idf scores (dictionary returned by tfidf). Function returns the top 5 documents from a collection of about 22k documents.
"""

def vectorSpaceModel(query, doc_dict,tfidf_scr):
    query_vocab = []
    for word in query.split():
        if word not in query_vocab:
            query_vocab.append(word)

    query_wc = {}
    for word in query_vocab:
        query_wc[word] = query.lower().split().count(word)
    
    relevance_scores = {}
    for doc_id in doc_dict.keys():
        score = 0
        for word in query_vocab:
            score += query_wc[word] * tfidf_scr[doc_id][word]
        relevance_scores[doc_id] = score
    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))
    top_5 = {k: sorted_value[k] for k in list(sorted_value)[:5]}
    return top_5

"""**Here is the main function call which calls all the functions above and gives the results.**"""

if __name__  == "__main__":
   docs = give_path("file.txt")                        #returns a dictionary of all docs
   M = len(docs)                                 #number of files in dataset
   w_List = wordList_removePuncs(docs)           #returns a list of tokenized words
   vocab = list(set(w_List))                     #returns a list of unique words
   tf_dict = termFrequencyInDoc(vocab, docs)     #returns term frequency
   df_dict = wordDocFre(vocab, docs)             #returns document frequencies
   idf_dict = inverseDocFre(vocab,df_dict,M)     #returns idf scores
   tf_idf = tfidf(vocab,tf_dict,idf_dict,docs)   #returns tf-idf socres
   
   query1 = 'professional'
   top1 = vectorSpaceModel(query1, docs,tf_idf)    #returns top 5 documents using VSM
   print('Top 5 Documents for Query 1: \n', top1)
   print('\n')

